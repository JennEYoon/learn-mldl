# My notes from Optimization  

#### alpha - learning rate
  * If alpha is too big from the start - can overshoot optimal minimum point.  
  * If alpha is too small from the start - takes too long to converge to solution.  

#### number of minibatches, L - number of layers  
  * twick number of minibatches, number of layers

#### Batch Normalization  

Z-values, every level per layer normalize. No one feature blows away other features.  
Change scale so no one feature dominates result with high values. 
